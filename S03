import pandas as pd
import numpy as nb

# Download the dataset
data_url = "https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv"
data = pd.read_csv(data_url)
# Select the required features and clean the column names
selected_features = [
    'Make', 'Model', 'Year', 'Engine HP', 'Engine Cylinders', 'Transmission Type',
    'Vehicle Style', 'highway MPG', 'city mpg', 'MSRP'
]
data = data[selected_features]
data.columns = data.columns.str.replace(' ', '_').str.lower()

# Fill missing values with 0
data.fillna(0, inplace=True)

# Rename MSRP to price
data.rename(columns={'msrp': 'price'}, inplace=True)
mode_transmission_type
correlation_matrix = data.corr()
# Identify the two features with the highest correlation
highest_correlation = correlation_matrix.unstack().sort_values(ascending=False)[2:4]
highest_correlation
# Calculate the mean of the 'price' variable
price_mean = data['price'].mean()

# Create the 'above_average' variable
data['above_average'] = (data['price'] > price_mean).astype(int)

from sklearn.model_selection import train_test_split

# Define the features (X) and the target variable (y)
X = data.drop(columns=['above_average'])  # Features (excluding the target variable)
y = data['above_average']  # Target variable

# Split the data into train/val/test sets with a 60%/20%/20% distribution
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Ensure that the target value is not in the data frame for validation and test sets
val_df = pd.DataFrame({'above_average': y_val})
test_df = pd.DataFrame({'above_average': y_test})
X_val = X_val.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)

# Print the shapes of the resulting datasets
print("Train set shape:", X_train.shape, y_train.shape)
print("Validation set shape:", X_val.shape, val_df.shape)
print("Test set shape:", X_test.shape, test_df.shape)

from sklearn.feature_selection import mutual_info_classif

# Define the categorical features
categorical_features = ['make', 'model', 'transmission_type', 'vehicle_style']

from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import LabelEncoder

# Initialize a LabelEncoder for encoding categorical variables
label_encoder = LabelEncoder()

# Encode categorical features in the training set
X_train_encoded = X_train[categorical_features].apply(lambda col: label_encoder.fit_transform(col))

# Calculate mutual information scores for categorical features using the training data
mi_scores = mutual_info_classif(X_train_encoded, y_train, discrete_features='auto', random_state=42)

# Round the scores to 2 decimal places
rounded_mi_scores = [round(score, 2) for score in mi_scores]

# Create a dictionary with variable names and their corresponding mutual information scores
mi_scores_dict = dict(zip(categorical_features, rounded_mi_scores))

# Find the variable with the lowest mutual information score
lowest_mi_variable = min(mi_scores_dict, key=mi_scores_dict.get)

# Print the scores and the variable with the lowest score
print("Mutual Information Scores for Categorical Variables:")
for feature, score in mi_scores_dict.items():
    print(f"{feature}: {score}")
print(f"Variable with the Lowest Score: {lowest_mi_variable}")
